\documentclass[12pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{listings}
\usepackage{xcolor}

% Page geometry
\geometry{margin=1in}

% Hyperref settings
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=blue,
}

% Title information
\title{\textbf{Gold Price Forecasting using Machine Learning}\\
\large A Financial Engineering Perspective}
\author{Felix Jouary\\
Financial Engineering\\
Machine Learning Project}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This report presents a comprehensive machine learning approach to gold price forecasting from a financial engineering perspective. Gold, as a critical asset in portfolio management and risk hedging, requires accurate price prediction models. We implemented and compared 10 machine learning models, ranging from linear regression to ensemble methods, achieving an RMSE of \$13.76 and R² of 0.9977 with the best model. The study demonstrates the effectiveness of regularized linear models for financial time series prediction and provides insights into feature engineering, model selection, and overfitting prevention in financial applications.
\end{abstract}

\tableofcontents
\newpage

\section{Business Case: Financial Engineering Context}

Gold has been a cornerstone asset in financial markets for centuries, serving multiple critical functions in modern portfolio management and risk hedging strategies. From a financial engineering perspective, accurate gold price forecasting is essential for several reasons:

\subsection{Portfolio Diversification and Risk Management}
Gold exhibits a low or negative correlation with traditional equity markets, making it an invaluable diversification tool. During periods of market stress or economic uncertainty, gold typically acts as a safe-haven asset, preserving capital when other asset classes decline. For portfolio managers and financial engineers, the ability to forecast gold prices enables optimal asset allocation decisions and dynamic hedging strategies.

\subsection{Inflation Hedge and Currency Risk}
As a tangible asset with intrinsic value, gold serves as a natural hedge against inflation and currency devaluation. Central banks hold significant gold reserves as part of their foreign exchange reserves. Financial institutions use gold futures and options to manage currency exposure and protect against purchasing power erosion. Accurate price forecasting models allow for more effective implementation of inflation-protected investment strategies.

\subsection{Derivatives Pricing and Trading Strategies}
The gold derivatives market, including futures, options, and ETFs, represents billions of dollars in daily trading volume. Financial engineers require sophisticated forecasting models to price these derivatives accurately, develop algorithmic trading strategies, and manage risk exposures. Machine learning approaches can capture complex non-linear patterns that traditional econometric models may miss.

\subsection{Project Objective}
This project aims to develop and compare multiple machine learning models for gold price prediction, evaluating their performance in terms of accuracy, generalization capability, and robustness. The goal is to identify the most effective modeling approach for financial time series forecasting in the context of gold price prediction, with potential applications to other precious metals and commodity markets.

\section{Dataset Description}

\subsection{Data Source and Structure}
The dataset consists of historical gold prices in USD from multiple timeframes (daily, weekly, monthly, quarterly, and yearly data). For this analysis, we primarily utilized the \texttt{Daily.csv} file, which provides the most granular view of gold price movements.

\subsection{Dataset Characteristics}
\begin{itemize}
    \item \textbf{Time Period}: Historical daily gold prices spanning multiple years
    \item \textbf{Total Observations}: 11,539 daily records
    \item \textbf{Features}: Date and Price (USD per troy ounce)
    \item \textbf{Data Quality}: No missing values detected in the primary dataset
    \item \textbf{Price Range}: Gold prices ranging from historical lows to recent highs, capturing multiple market cycles
\end{itemize}

\subsection{Feature Engineering}
Given the temporal nature of financial time series, we engineered 28 features from the original price data:

\begin{enumerate}
    \item \textbf{Temporal Features}: Year, Month, Day, DayOfWeek, Quarter, WeekOfYear
    \item \textbf{Lag Features}: Price lags at 1, 2, 3, 5, 7, 14, 21, and 30 days
    \item \textbf{Moving Averages}: 7-day, 14-day, 30-day, 60-day, and 90-day moving averages
    \item \textbf{Returns}: 1-day, 5-day, and 30-day percentage returns
    \item \textbf{Volatility Indicators}: 7-day and 30-day rolling standard deviations
    \item \textbf{Technical Indicators}: Price vs MA7, Price vs MA30, Rolling Min/Max over 30 days
\end{enumerate}

These features capture temporal patterns, momentum, trend information, and market volatility—all critical factors in financial time series prediction.

\subsection{Data Splitting}
To respect the temporal ordering inherent in time series data, we employed a chronological split:
\begin{itemize}
    \item \textbf{Training Set}: 80\% of the data (9,231 observations)
    \item \textbf{Test Set}: 20\% of the data (2,308 observations)
\end{itemize}

All features were standardized using StandardScaler to ensure that models with distance-based metrics (like SVR) perform optimally.

\section{Exploratory Data Analysis}

\subsection{Price Trends and Patterns}
The exploratory analysis revealed several key patterns in gold price behavior:

\begin{itemize}
    \item \textbf{Long-term Upward Trend}: Gold prices exhibit a general upward trend over the historical period, reflecting inflation, currency devaluation, and increased demand.
    \item \textbf{Volatility Clustering}: Periods of high volatility tend to cluster together, particularly during economic crises (2008 financial crisis, COVID-19 pandemic).
    \item \textbf{Seasonality}: Weak seasonal patterns were observed, with some evidence of increased volatility during specific quarters related to central bank policy announcements.
\end{itemize}

\subsection{Statistical Properties}
\begin{itemize}
    \item \textbf{Distribution}: Gold prices show a right-skewed distribution, with the majority of observations concentrated around the mean.
    \item \textbf{Stationarity}: The raw price series is non-stationary, which motivated the inclusion of differenced features (returns) and moving averages.
    \item \textbf{Outliers}: Several outliers were identified corresponding to major market events, but were retained as they represent genuine market dynamics.
\end{itemize}

\subsection{Feature Correlations}
Correlation analysis revealed:
\begin{itemize}
    \item Strong correlations between price and its recent lags (especially 1-day and 2-day lags)
    \item High correlation between price and medium-term moving averages (MA30, MA60, MA90)
    \item Moderate negative correlation between short-term returns and volatility measures
    \item Temporal features (Year, Month) showed weak but meaningful correlations with price
\end{itemize}

These findings informed our feature selection and guided the modeling approach, emphasizing the importance of recent price history and trend indicators.

\section{Model Descriptions}

We implemented and compared 10 machine learning models, divided into baseline and advanced categories. All models were trained using hyperparameter tuning via GridSearchCV with TimeSeriesSplit cross-validation to respect temporal ordering.

\subsection{Baseline Models}

\subsubsection{Linear Regression}
The simplest model, fitting a linear relationship between features and target:
\begin{equation}
\hat{y} = \beta_0 + \sum_{i=1}^{n} \beta_i x_i
\end{equation}
where $\beta_i$ are the coefficients estimated via ordinary least squares.

\subsubsection{Ridge Regression (L2 Regularization)}
Adds L2 penalty to prevent overfitting:
\begin{equation}
\min_{\beta} \left\{ \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \alpha \sum_{j=1}^{p} \beta_j^2 \right\}
\end{equation}
Hyperparameter $\alpha$ controls regularization strength (tested: 0.1, 1.0, 10.0, 100.0).

\subsubsection{Lasso Regression (L1 Regularization)}
Uses L1 penalty for feature selection:
\begin{equation}
\min_{\beta} \left\{ \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \alpha \sum_{j=1}^{p} |\beta_j| \right\}
\end{equation}
Can drive some coefficients to exactly zero, performing implicit feature selection.

\subsubsection{Decision Tree}
Recursively partitions the feature space using binary splits to minimize MSE at each node. Hyperparameters tuned: max\_depth (5, 10, 20, None), min\_samples\_split (2, 5, 10).

\subsubsection{Random Forest}
Ensemble of decision trees trained on bootstrap samples with random feature subsets:
\begin{equation}
\hat{y} = \frac{1}{B} \sum_{b=1}^{B} T_b(x)
\end{equation}
where $T_b$ represents individual trees. Tuned: n\_estimators (100, 200, 300), max\_depth (10, 20, None).

\subsubsection{Gradient Boosting}
Sequential ensemble that fits new trees to residuals:
\begin{equation}
F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x)
\end{equation}
where $\nu$ is the learning rate and $h_m$ is a weak learner. Tuned: n\_estimators (100, 200), learning\_rate (0.01, 0.1), max\_depth (3, 5).

\subsection{Advanced Models}

\subsubsection{Support Vector Regression (SVR)}
Maps data to high-dimensional space using RBF kernel:
\begin{equation}
\min_{w,b} \frac{1}{2} ||w||^2 + C \sum_{i=1}^{n} \max(0, |y_i - \hat{y}_i| - \epsilon)
\end{equation}
Tuned: C (1, 10, 100), kernel (rbf), epsilon (0.1, 0.2).

\subsubsection{AdaBoost}
Adaptive boosting that adjusts sample weights based on errors:
\begin{equation}
F(x) = \sum_{m=1}^{M} \alpha_m h_m(x)
\end{equation}
Tuned: n\_estimators (50, 100, 200), learning\_rate (0.01, 0.1, 1.0).

\subsubsection{Extra Trees (Extremely Randomized Trees)}
Similar to Random Forest but uses random thresholds for splits, adding more randomization. Tuned: n\_estimators (100, 200), max\_depth (10, 20, None), min\_samples\_split (2, 5).

\subsubsection{Bagging Regressor}
Bootstrap aggregating with base estimators trained on random subsets:
\begin{equation}
\hat{y} = \frac{1}{M} \sum_{m=1}^{M} f_m(x)
\end{equation}
Tuned: n\_estimators (10, 50, 100), max\_samples (0.5, 0.8, 1.0).

\section{Obstacles Encountered}

\subsection{Library Compatibility Issues}
Initial attempts to implement XGBoost and LightGBM on macOS encountered significant challenges:

\subsubsection{OpenMP Library Conflicts}
Both XGBoost and LightGBM require OpenMP for parallel processing. On macOS, the \texttt{libomp.dylib} library conflicts arose between pip and conda installations, resulting in runtime errors even after successful package installation. Multiple resolution attempts (conda install, brew install llvm) failed to resolve version mismatches.

\subsubsection{Model Substitution Strategy}
Rather than compromise the project timeline, we pivoted to equivalent sklearn-based ensemble models:
\begin{itemize}
    \item Replaced XGBoost with \textbf{Extra Trees} (similar randomized ensemble approach)
    \item Replaced LightGBM with \textbf{Bagging Regressor} (bootstrap aggregation)
    \item Maintained \textbf{Gradient Boosting} from sklearn as an alternative boosting method
\end{itemize}

This substitution maintained the diversity of modeling approaches while ensuring reproducibility across platforms.

\subsection{Computational Constraints}
\subsubsection{SVR Training Time}
Support Vector Regression with RBF kernel has $O(n^2)$ to $O(n^3)$ complexity. With 9,231 training samples, GridSearchCV became prohibitively slow. We addressed this by:
\begin{itemize}
    \item Using a subset (3,000 samples) for hyperparameter search
    \item Training the final model on the full dataset with optimal parameters
\end{itemize}

\subsubsection{Cross-Validation Strategy}
Standard k-fold cross-validation violates temporal ordering in time series. We implemented \textbf{TimeSeriesSplit} (5 folds) to ensure that training data always precedes validation data, preventing data leakage.

\subsection{Overfitting in Tree-Based Models}
Initial Decision Tree and Random Forest models showed severe overfitting (R² = 0.9999 on training, 0.97 on test). This was mitigated by:
\begin{itemize}
    \item Constraining max\_depth and min\_samples\_split
    \item Using ensemble methods (Random Forest, Bagging) for variance reduction
    \item Evaluating the overfitting ratio (Test RMSE / Train RMSE) as a diagnostic metric
\end{itemize}

\subsection{IDE Caching Issues}
VSCode's notebook kernel occasionally cached outdated file versions, causing confusion during iterative development. This was resolved by explicitly closing and reopening notebooks after file modifications.

\section{Results and Comparison}

\subsection{Model Performance Metrics}

Table \ref{tab:results} presents the comprehensive performance comparison of all 10 models on the test set, sorted by RMSE.

\begin{table}[H]
\centering
\caption{Model Performance Comparison on Test Set}
\label{tab:results}
\begin{tabular}{lccccl}
\toprule
\textbf{Model} & \textbf{RMSE (\$)} & \textbf{MAE (\$)} & \textbf{MAPE (\%)} & \textbf{R²} & \textbf{Type} \\
\midrule
Lasso Regression & 13.76 & 9.33 & 0.62 & 0.9977 & Baseline \\
Ridge Regression & 13.77 & 9.40 & 0.62 & 0.9977 & Baseline \\
Linear Regression & 13.77 & 9.40 & 0.62 & 0.9977 & Baseline \\
Decision Tree & 46.18 & 29.05 & 1.78 & 0.9745 & Baseline \\
Random Forest & 50.76 & 28.23 & 1.64 & 0.9691 & Baseline \\
Bagging & 51.39 & 28.58 & 1.65 & 0.9684 & Advanced \\
AdaBoost & 53.12 & 32.49 & 1.98 & 0.9662 & Advanced \\
Gradient Boosting & 60.87 & 36.16 & 2.10 & 0.9556 & Baseline \\
Extra Trees & 64.08 & 36.31 & 2.07 & 0.9508 & Advanced \\
SVR & 120.22 & 68.04 & 3.80 & 0.8268 & Advanced \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Key Findings}

\subsubsection{Linear Models Dominate}
Surprisingly, regularized linear models (Lasso, Ridge) achieved the best performance with nearly identical metrics:
\begin{itemize}
    \item \textbf{RMSE}: \$13.76-\$13.77 (less than 0.1\% difference)
    \item \textbf{R²}: 0.9977 (explaining 99.77\% of variance)
    \item \textbf{MAPE}: 0.62\% (highly accurate percentage error)
\end{itemize}

This suggests that the engineered features (lags, moving averages, returns) capture the underlying linear relationships effectively, and that gold price movements exhibit strong linear predictability at this granularity.

\subsubsection{Tree-Based Models Show Moderate Performance}
Ensemble tree methods performed well but lagged behind linear models:
\begin{itemize}
    \item Random Forest and Bagging achieved R² $\approx$ 0.97
    \item Decision Tree alone (R² = 0.9745) outperformed some ensemble methods
    \item Gradient Boosting and Extra Trees underperformed expectations
\end{itemize}

\subsubsection{SVR Struggles with Scale}
Despite hyperparameter tuning, SVR showed the worst performance (RMSE = \$120.22, R² = 0.8268). This is likely due to:
\begin{itemize}
    \item High-dimensional feature space (28 features)
    \item Large training set size reducing kernel effectiveness
    \item Sensitivity to hyperparameter selection
\end{itemize}

\subsection{Overfitting Analysis}

Table \ref{tab:overfit} compares training vs. test performance to assess generalization.

\begin{table}[H]
\centering
\caption{Overfitting Analysis: Train vs Test RMSE}
\label{tab:overfit}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Train RMSE (\$)} & \textbf{Test RMSE (\$)} & \textbf{Overfit Ratio} \\
\midrule
Linear Regression & 8.77 & 13.77 & 1.57 \\
AdaBoost & 25.77 & 53.12 & 2.06 \\
SVR & 13.92 & 120.22 & 8.64 \\
Extra Trees & 5.22 & 64.08 & 12.27 \\
Random Forest & 3.99 & 50.76 & 12.73 \\
Bagging & 3.53 & 51.39 & 14.56 \\
Decision Tree & 3.05 & 46.18 & 15.14 \\
Gradient Boosting & 3.52 & 60.87 & 17.30 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Interpretation}:
\begin{itemize}
    \item Overfitting ratio close to 1.0 indicates good generalization
    \item Linear models show minimal overfitting (ratio = 1.57)
    \item Tree-based models exhibit severe overfitting (ratios $>$ 10), achieving near-perfect training fit but poor test generalization
    \item This explains why simpler linear models outperform complex tree ensembles
\end{itemize}

\subsection{Best Model Selection}

\textbf{Winner: Lasso Regression}

The Lasso model is selected as the optimal choice for gold price forecasting based on:
\begin{enumerate}
    \item \textbf{Best Test Performance}: RMSE = \$13.76, R² = 0.9977, MAPE = 0.62\%
    \item \textbf{Minimal Overfitting}: Overfit ratio = 1.57 (acceptable generalization)
    \item \textbf{Interpretability}: Linear coefficients allow understanding of feature importance
    \item \textbf{Computational Efficiency}: Fast training and prediction
    \item \textbf{Feature Selection}: L1 regularization automatically identifies most relevant features
\end{enumerate}

The Lasso model's regularization prevented overfitting while maintaining excellent predictive accuracy, making it suitable for production deployment in financial applications.

\section{Conclusion}

This project successfully developed and compared 10 machine learning models for gold price forecasting from a financial engineering perspective. The comprehensive analysis yielded several important insights:

\subsection{Main Findings}

\begin{enumerate}
    \item \textbf{Linear Models Excel}: Contrary to the common assumption that complex models outperform simpler ones, regularized linear regression (Lasso, Ridge) achieved the best performance with RMSE = \$13.76 and R² = 0.9977. This demonstrates the power of effective feature engineering combined with appropriate regularization.

    \item \textbf{Feature Engineering is Critical}: The 28 engineered features—including temporal patterns, lag features, moving averages, returns, and volatility indicators—successfully captured the underlying dynamics of gold price movements, enabling strong linear predictability.

    \item \textbf{Overfitting Risk in Complex Models}: Tree-based ensemble methods, despite their popularity in machine learning, suffered from severe overfitting (overfit ratios $>$ 10), achieving near-perfect training performance but significantly worse test performance than linear models.

    \item \textbf{Simplicity and Interpretability Matter}: In financial applications, model interpretability is crucial for regulatory compliance and stakeholder trust. Linear models provide clear coefficient interpretations, making them preferable to black-box alternatives when performance is comparable or superior.
\end{enumerate}

\subsection{Practical Implications for Financial Engineering}

For practitioners in financial engineering and quantitative finance:
\begin{itemize}
    \item \textbf{Portfolio Management}: The \$13.76 RMSE represents approximately 0.62\% MAPE, providing sufficient accuracy for tactical asset allocation decisions and rebalancing strategies.

    \item \textbf{Risk Management}: The model's R² of 0.9977 indicates reliable prediction intervals for Value-at-Risk (VaR) and Expected Shortfall calculations in portfolios containing gold exposure.

    \item \textbf{Derivatives Trading}: While not suitable for high-frequency trading due to daily granularity, the model supports medium-term options pricing and hedging strategies.

    \item \textbf{Real-time Deployment}: Lasso regression's computational efficiency enables real-time predictions with minimal latency, suitable for production environments.
\end{itemize}

\subsection{Limitations and Future Work}

\begin{itemize}
    \item \textbf{Exogenous Variables}: The model relies solely on historical price data. Incorporating macroeconomic indicators (USD strength, inflation rates, interest rates) could improve performance.

    \item \textbf{Market Regime Changes}: The model assumes stationary relationships. Future work could implement regime-switching models to adapt to changing market conditions.

    \item \textbf{Deep Learning}: While we encountered technical challenges with LSTM implementation, neural networks may capture long-term dependencies that linear models miss. This warrants future investigation with proper computational resources.

    \item \textbf{Multi-Asset Extension}: The methodology could be extended to other precious metals (silver, platinum, palladium) and cross-asset relationships.
\end{itemize}

\subsection{Final Remarks}

This project demonstrates that in financial time series forecasting, sophisticated feature engineering combined with regularized linear models can outperform complex ensemble methods. The Lasso regression model achieves production-ready accuracy while maintaining interpretability—a critical requirement in financial engineering applications. The systematic comparison of 10 models provides a robust framework for model selection in similar forecasting tasks, emphasizing the importance of validation methodology, overfitting detection, and practical deployment considerations.

\section{References}
\begin{thebibliography}{99}

\bibitem{ridge1970}
Hoerl, A. E., \& Kennard, R. W. (1970). Ridge regression: Biased estimation for nonorthogonal problems. \textit{Technometrics}, 12(1), 55-67.

\bibitem{lasso1996}
Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. \textit{Journal of the Royal Statistical Society: Series B (Methodological)}, 58(1), 267-288.

\bibitem{breiman1996}
Breiman, L. (1996). Bagging predictors. \textit{Machine Learning}, 24(2), 123-140.

\bibitem{breiman2001}
Breiman, L. (2001). Random forests. \textit{Machine Learning}, 45(1), 5-32.

\bibitem{freund1997}
Freund, Y., \& Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. \textit{Journal of Computer and System Sciences}, 55(1), 119-139.

\bibitem{friedman2001}
Friedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. \textit{Annals of Statistics}, 1189-1232.

\bibitem{drucker1997}
Drucker, H., Burges, C. J., Kaufman, L., Smola, A., \& Vapnik, V. (1997). Support vector regression machines. \textit{Advances in Neural Information Processing Systems}, 9, 155-161.

\bibitem{geurts2006}
Geurts, P., Ernst, D., \& Wehenkel, L. (2006). Extremely randomized trees. \textit{Machine Learning}, 63(1), 3-42.

\bibitem{sklearn}
Pedregosa, F., et al. (2011). Scikit-learn: Machine learning in Python. \textit{Journal of Machine Learning Research}, 12, 2825-2830.

\end{thebibliography}

\end{document}
