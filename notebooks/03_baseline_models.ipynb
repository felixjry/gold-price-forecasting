{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Baseline Models - Gold Price Forecasting\n",
    "\n",
    "**Objective:** Implement and evaluate baseline machine learning models.\n",
    "\n",
    "**Author:** Félix Jouary  \n",
    "**Dataset:** Kaggle Gold Price Dataset\n",
    "\n",
    "**Models in this notebook:**\n",
    "- Linear Regression (baseline)\n",
    "- Ridge Regression\n",
    "- Lasso Regression\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scaled data\n",
    "X_train_scaled = np.load('../data/processed/X_train_scaled.npy')\n",
    "X_test_scaled = np.load('../data/processed/X_test_scaled.npy')\n",
    "y_train = np.load('../data/processed/y_train.npy')\n",
    "y_test = np.load('../data/processed/y_test.npy')\n",
    "\n",
    "# Load feature names\n",
    "feature_names = pd.read_csv('../data/processed/feature_names.csv').iloc[:, 0].tolist()\n",
    "\n",
    "# Load original data for visualization\n",
    "train_data = pd.read_csv('../data/processed/train_data.csv', parse_dates=['Date'])\n",
    "test_data = pd.read_csv('../data/processed/test_data.csv', parse_dates=['Date'])\n",
    "\n",
    "print(f\"X_train shape: {X_train_scaled.shape}\")\n",
    "print(f\"X_test shape: {X_test_scaled.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"\\nNumber of features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Define Evaluation Metrics\n",
    "\n",
    "For regression problems, we use:\n",
    "- **RMSE** (Root Mean Squared Error): Penalizes large errors\n",
    "- **MAE** (Mean Absolute Error): Average error magnitude\n",
    "- **MAPE** (Mean Absolute Percentage Error): Percentage error\n",
    "- **R²** (Coefficient of Determination): Explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"Calculate and display regression metrics.\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{model_name} - Evaluation Metrics\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"RMSE:  ${rmse:.2f}\")\n",
    "    print(f\"MAE:   ${mae:.2f}\")\n",
    "    print(f\"MAPE:  {mape:.2f}%\")\n",
    "    print(f\"R²:    {r2:.4f}\")\n",
    "    \n",
    "    return {'Model': model_name, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape, 'R2': r2}\n",
    "\n",
    "# Store results for comparison\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Model 1: Linear Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr_train = lr_model.predict(X_train_scaled)\n",
    "y_pred_lr_test = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "print(\"TRAINING SET:\")\n",
    "_ = evaluate_model(y_train, y_pred_lr_train, \"Linear Regression (Train)\")\n",
    "\n",
    "print(\"\\nTEST SET:\")\n",
    "lr_results = evaluate_model(y_test, y_pred_lr_test, \"Linear Regression\")\n",
    "results.append(lr_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Model 2: Ridge Regression (L2 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Ridge\n",
    "ridge_params = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Time Series Cross-Validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "ridge_grid = GridSearchCV(\n",
    "    Ridge(),\n",
    "    ridge_params,\n",
    "    cv=tscv,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "ridge_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best alpha: {ridge_grid.best_params_['alpha']}\")\n",
    "print(f\"Best CV score (neg MSE): {ridge_grid.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with best parameters\n",
    "ridge_model = ridge_grid.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_pred_ridge_train = ridge_model.predict(X_train_scaled)\n",
    "y_pred_ridge_test = ridge_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "print(\"TRAINING SET:\")\n",
    "_ = evaluate_model(y_train, y_pred_ridge_train, \"Ridge Regression (Train)\")\n",
    "\n",
    "print(\"\\nTEST SET:\")\n",
    "ridge_results = evaluate_model(y_test, y_pred_ridge_test, \"Ridge Regression\")\n",
    "results.append(ridge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Model 3: Lasso Regression (L1 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Lasso\n",
    "lasso_params = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "lasso_grid = GridSearchCV(\n",
    "    Lasso(max_iter=10000),\n",
    "    lasso_params,\n",
    "    cv=tscv,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lasso_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best alpha: {lasso_grid.best_params_['alpha']}\")\n",
    "print(f\"Best CV score (neg MSE): {lasso_grid.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with best parameters\n",
    "lasso_model = lasso_grid.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_pred_lasso_train = lasso_model.predict(X_train_scaled)\n",
    "y_pred_lasso_test = lasso_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "print(\"TRAINING SET:\")\n",
    "_ = evaluate_model(y_train, y_pred_lasso_train, \"Lasso Regression (Train)\")\n",
    "\n",
    "print(\"\\nTEST SET:\")\n",
    "lasso_results = evaluate_model(y_test, y_pred_lasso_test, \"Lasso Regression\")\n",
    "results.append(lasso_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Lasso (non-zero coefficients)\n",
    "lasso_coef = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': lasso_model.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"Top 10 features by Lasso coefficient magnitude:\")\n",
    "print(lasso_coef.head(10))\n",
    "\n",
    "# Count non-zero features\n",
    "non_zero = (lasso_coef['Coefficient'] != 0).sum()\n",
    "print(f\"\\nFeatures selected by Lasso: {non_zero}/{len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Model 4: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Decision Tree\n",
    "dt_params = {\n",
    "    'max_depth': [5, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "dt_grid = GridSearchCV(\n",
    "    DecisionTreeRegressor(random_state=42),\n",
    "    dt_params,\n",
    "    cv=tscv,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "dt_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best parameters: {dt_grid.best_params_}\")\n",
    "print(f\"Best CV score (neg MSE): {dt_grid.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with best parameters\n",
    "dt_model = dt_grid.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_pred_dt_train = dt_model.predict(X_train_scaled)\n",
    "y_pred_dt_test = dt_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "print(\"TRAINING SET:\")\n",
    "_ = evaluate_model(y_train, y_pred_dt_train, \"Decision Tree (Train)\")\n",
    "\n",
    "print(\"\\nTEST SET:\")\n",
    "dt_results = evaluate_model(y_test, y_pred_dt_test, \"Decision Tree\")\n",
    "results.append(dt_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Model 5: Random Forest (Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Random Forest\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 15, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    rf_params,\n",
    "    cv=tscv,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {rf_grid.best_params_}\")\n",
    "print(f\"Best CV score (neg MSE): {rf_grid.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with best parameters\n",
    "rf_model = rf_grid.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf_train = rf_model.predict(X_train_scaled)\n",
    "y_pred_rf_test = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "print(\"TRAINING SET:\")\n",
    "_ = evaluate_model(y_train, y_pred_rf_train, \"Random Forest (Train)\")\n",
    "\n",
    "print(\"\\nTEST SET:\")\n",
    "rf_results = evaluate_model(y_test, y_pred_rf_test, \"Random Forest\")\n",
    "results.append(rf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot top 15 features\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(rf_importance['Feature'].head(15)[::-1], rf_importance['Importance'].head(15)[::-1])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 15 Feature Importances - Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/rf_feature_importance.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 most important features:\")\n",
    "print(rf_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Model 6: Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Gradient Boosting\n",
    "gb_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "gb_grid = GridSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    gb_params,\n",
    "    cv=tscv,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "gb_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {gb_grid.best_params_}\")\n",
    "print(f\"Best CV score (neg MSE): {gb_grid.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with best parameters\n",
    "gb_model = gb_grid.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_pred_gb_train = gb_model.predict(X_train_scaled)\n",
    "y_pred_gb_test = gb_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "print(\"TRAINING SET:\")\n",
    "_ = evaluate_model(y_train, y_pred_gb_train, \"Gradient Boosting (Train)\")\n",
    "\n",
    "print(\"\\nTEST SET:\")\n",
    "gb_results = evaluate_model(y_test, y_pred_gb_test, \"Gradient Boosting\")\n",
    "results.append(gb_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10 Overfitting Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare train vs test performance to detect overfitting\n",
    "models_names = ['Linear Regression', 'Ridge', 'Lasso', 'Decision Tree', 'Random Forest', 'Gradient Boosting']\n",
    "train_preds = [y_pred_lr_train, y_pred_ridge_train, y_pred_lasso_train, y_pred_dt_train, y_pred_rf_train, y_pred_gb_train]\n",
    "test_preds = [y_pred_lr_test, y_pred_ridge_test, y_pred_lasso_test, y_pred_dt_test, y_pred_rf_test, y_pred_gb_test]\n",
    "\n",
    "overfitting_analysis = []\n",
    "for name, train_pred, test_pred in zip(models_names, train_preds, test_preds):\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "    gap = ((test_rmse - train_rmse) / train_rmse) * 100\n",
    "    \n",
    "    overfitting_analysis.append({\n",
    "        'Model': name,\n",
    "        'Train RMSE': train_rmse,\n",
    "        'Test RMSE': test_rmse,\n",
    "        'Gap (%)': gap\n",
    "    })\n",
    "\n",
    "overfit_df = pd.DataFrame(overfitting_analysis)\n",
    "print(\"Overfitting Analysis (Train vs Test RMSE):\")\n",
    "print(overfit_df.to_string(index=False))\n",
    "print(\"\\nNote: Large positive gap indicates overfitting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize overfitting\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(models_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, overfit_df['Train RMSE'], width, label='Train RMSE')\n",
    "bars2 = ax.bar(x + width/2, overfit_df['Test RMSE'], width, label='Test RMSE')\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('RMSE ($)')\n",
    "ax.set_title('Train vs Test RMSE - Overfitting Analysis')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models_names, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/overfitting_analysis.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.11 Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('RMSE')\n",
    "\n",
    "print(\"Model Comparison (sorted by RMSE):\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# RMSE\n",
    "axes[0, 0].barh(results_df['Model'], results_df['RMSE'], color='steelblue')\n",
    "axes[0, 0].set_xlabel('RMSE ($)')\n",
    "axes[0, 0].set_title('RMSE by Model (lower is better)')\n",
    "\n",
    "# MAE\n",
    "axes[0, 1].barh(results_df['Model'], results_df['MAE'], color='coral')\n",
    "axes[0, 1].set_xlabel('MAE ($)')\n",
    "axes[0, 1].set_title('MAE by Model (lower is better)')\n",
    "\n",
    "# MAPE\n",
    "axes[1, 0].barh(results_df['Model'], results_df['MAPE'], color='seagreen')\n",
    "axes[1, 0].set_xlabel('MAPE (%)')\n",
    "axes[1, 0].set_title('MAPE by Model (lower is better)')\n",
    "\n",
    "# R²\n",
    "axes[1, 1].barh(results_df['Model'], results_df['R2'], color='purple')\n",
    "axes[1, 1].set_xlabel('R²')\n",
    "axes[1, 1].set_title('R² by Model (higher is better)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/model_comparison_baseline.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.12 Predictions Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted for best model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "\n",
    "# Get predictions from best model\n",
    "if best_model_name == 'Random Forest':\n",
    "    best_pred = y_pred_rf_test\n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    best_pred = y_pred_gb_test\n",
    "elif best_model_name == 'Ridge Regression':\n",
    "    best_pred = y_pred_ridge_test\n",
    "elif best_model_name == 'Linear Regression':\n",
    "    best_pred = y_pred_lr_test\n",
    "elif best_model_name == 'Lasso Regression':\n",
    "    best_pred = y_pred_lasso_test\n",
    "else:\n",
    "    best_pred = y_pred_dt_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(test_data['Date'], y_test, label='Actual', alpha=0.8)\n",
    "plt.plot(test_data['Date'], best_pred, label=f'Predicted ({best_model_name})', alpha=0.8)\n",
    "plt.title(f'Gold Price Prediction - {best_model_name}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price (USD)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/best_model_predictions.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Actual vs Predicted\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(y_test, best_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label='Perfect Prediction')\n",
    "plt.xlabel('Actual Price (USD)')\n",
    "plt.ylabel('Predicted Price (USD)')\n",
    "plt.title(f'Actual vs Predicted - {best_model_name}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/actual_vs_predicted.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals analysis\n",
    "residuals = y_test - best_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Residuals over time\n",
    "axes[0].plot(test_data['Date'], residuals)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0].set_title('Residuals Over Time')\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Residual (USD)')\n",
    "\n",
    "# Residuals distribution\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black')\n",
    "axes[1].axvline(x=0, color='r', linestyle='--')\n",
    "axes[1].set_title('Residuals Distribution')\n",
    "axes[1].set_xlabel('Residual (USD)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/residuals_analysis.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Residuals Mean: {residuals.mean():.2f}\")\n",
    "print(f\"Residuals Std: {residuals.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.13 Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all models\n",
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "joblib.dump(lr_model, '../models/linear_regression.pkl')\n",
    "joblib.dump(ridge_model, '../models/ridge_regression.pkl')\n",
    "joblib.dump(lasso_model, '../models/lasso_regression.pkl')\n",
    "joblib.dump(dt_model, '../models/decision_tree.pkl')\n",
    "joblib.dump(rf_model, '../models/random_forest.pkl')\n",
    "joblib.dump(gb_model, '../models/gradient_boosting.pkl')\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('../reports/baseline_results.csv', index=False)\n",
    "\n",
    "print(\"All models saved successfully!\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"- ../models/linear_regression.pkl\")\n",
    "print(\"- ../models/ridge_regression.pkl\")\n",
    "print(\"- ../models/lasso_regression.pkl\")\n",
    "print(\"- ../models/decision_tree.pkl\")\n",
    "print(\"- ../models/random_forest.pkl\")\n",
    "print(\"- ../models/gradient_boosting.pkl\")\n",
    "print(\"- ../reports/baseline_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.14 Summary\n",
    "\n",
    "### Baseline Models Implemented:\n",
    "\n",
    "1. **Linear Regression** - Simple baseline\n",
    "2. **Ridge Regression** - L2 regularization to prevent overfitting\n",
    "3. **Lasso Regression** - L1 regularization for feature selection\n",
    "4. **Decision Tree** - Non-linear model\n",
    "5. **Random Forest** - Ensemble of decision trees\n",
    "6. **Gradient Boosting** - Sequential ensemble learning\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "- All models use **TimeSeriesSplit** for cross-validation (respects temporal order)\n",
    "- Hyperparameters were tuned using **GridSearchCV**\n",
    "- Overfitting analysis performed comparing train vs test performance\n",
    "- Feature importance analyzed for tree-based models\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Implement advanced models (XGBoost, LSTM)\n",
    "- Try dimensionality reduction\n",
    "- Final model comparison and selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*60)\n",
    "print(\"BASELINE MODELS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBest Model: {results_df.iloc[0]['Model']}\")\n",
    "print(f\"Best RMSE: ${results_df.iloc[0]['RMSE']:.2f}\")\n",
    "print(f\"Best MAE: ${results_df.iloc[0]['MAE']:.2f}\")\n",
    "print(f\"Best MAPE: {results_df.iloc[0]['MAPE']:.2f}%\")\n",
    "print(f\"Best R²: {results_df.iloc[0]['R2']:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
