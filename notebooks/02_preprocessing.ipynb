{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing - Gold Price Forecasting\n",
    "\n",
    "**Objective:** Clean, transform, and prepare data for machine learning models.\n",
    "\n",
    "**Author:** FÃ©lix Jouary  \n",
    "**Dataset:** Kaggle Gold Price Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the daily dataset\n",
    "df = pd.read_csv('../data/raw/Daily.csv', na_values=['#N/A', '#N/A N/A', 'N/A'])\n",
    "\n",
    "# Fix numeric columns - remove commas and convert to float\n",
    "for col in df.columns:\n",
    "    if col != 'Date':\n",
    "        df[col] = df[col].replace({',': ''}, regex=True)\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Convert Date column\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y')\n",
    "\n",
    "# Sort by date\n",
    "df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Focus on USD Price (Target Variable)\n",
    "\n",
    "For this project, we will focus on predicting the **Gold Price in USD**. Other currencies are highly correlated and would add complexity without significant value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a clean dataframe with Date and USD only\n",
    "df_gold = df[['Date', 'USD']].copy()\n",
    "df_gold.columns = ['Date', 'Price']\n",
    "\n",
    "print(f\"Working dataset shape: {df_gold.shape}\")\n",
    "print(f\"\\nDate range: {df_gold['Date'].min()} to {df_gold['Date'].max()}\")\n",
    "print(f\"\\nMissing values: {df_gold['Price'].isnull().sum()}\")\n",
    "df_gold.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check for missing values\nprint(\"Missing values before treatment:\")\nprint(df_gold.isnull().sum())\n\n# For time series, forward fill is appropriate (use previous day's price)\ndf_gold['Price'] = df_gold['Price'].ffill()\n\n# If any remaining NaN at the beginning, backward fill\ndf_gold['Price'] = df_gold['Price'].bfill()\n\nprint(\"\\nMissing values after treatment:\")\nprint(df_gold.isnull().sum())\n\n# Check for duplicates\nprint(\"\\n\" + \"=\"*50)\nprint(\"DUPLICATE CHECK\")\nprint(\"=\"*50)\nduplicates_date = df_gold[df_gold.duplicated(subset=['Date'], keep=False)]\nduplicates_full = df_gold[df_gold.duplicated(keep=False)]\n\nprint(f\"Duplicate dates: {len(duplicates_date)}\")\nprint(f\"Fully duplicate rows: {len(duplicates_full)}\")\n\nif len(duplicates_date) > 0:\n    print(\"\\nRemoving duplicate dates (keeping first occurrence)...\")\n    df_gold = df_gold.drop_duplicates(subset=['Date'], keep='first').reset_index(drop=True)\n    print(f\"Shape after removing duplicates: {df_gold.shape}\")\nelse:\n    print(\"No duplicates found - data is clean!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Outlier Detection and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IQR for outlier detection\n",
    "Q1 = df_gold['Price'].quantile(0.25)\n",
    "Q3 = df_gold['Price'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df_gold[(df_gold['Price'] < lower_bound) | (df_gold['Price'] > upper_bound)]\n",
    "\n",
    "print(f\"IQR Method:\")\n",
    "print(f\"Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}\")\n",
    "print(f\"Lower bound: {lower_bound:.2f}, Upper bound: {upper_bound:.2f}\")\n",
    "print(f\"\\nNumber of outliers detected: {len(outliers)} ({len(outliers)/len(df_gold)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Boxplot\n",
    "axes[0].boxplot(df_gold['Price'].values)\n",
    "axes[0].set_title('Boxplot - Gold Price (USD)')\n",
    "axes[0].set_ylabel('Price (USD)')\n",
    "\n",
    "# Time series with outliers highlighted\n",
    "axes[1].plot(df_gold['Date'], df_gold['Price'], label='Price', alpha=0.7)\n",
    "axes[1].scatter(outliers['Date'], outliers['Price'], color='red', label='Outliers', s=10)\n",
    "axes[1].set_title('Gold Price with Outliers Highlighted')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Price (USD)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/outliers_analysis.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: For gold prices, \"outliers\" are actually valid high prices during bull markets\n",
    "# We should NOT remove them as they represent real market conditions\n",
    "# This is a key insight for financial time series\n",
    "\n",
    "print(\"Decision: Keep all data points.\")\n",
    "print(\"Reason: In financial time series, extreme values are valid market conditions,\")\n",
    "print(\"not measurement errors. Removing them would bias our model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Feature Engineering for Time Series\n",
    "\n",
    "Creating features that capture temporal patterns and financial indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy for feature engineering\n",
    "df_features = df_gold.copy()\n",
    "\n",
    "# 1. Temporal Features\n",
    "df_features['Year'] = df_features['Date'].dt.year\n",
    "df_features['Month'] = df_features['Date'].dt.month\n",
    "df_features['Day'] = df_features['Date'].dt.day\n",
    "df_features['DayOfWeek'] = df_features['Date'].dt.dayofweek\n",
    "df_features['Quarter'] = df_features['Date'].dt.quarter\n",
    "df_features['WeekOfYear'] = df_features['Date'].dt.isocalendar().week.astype(int)\n",
    "\n",
    "print(\"Temporal features created!\")\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Lag Features (previous days' prices)\n",
    "for lag in [1, 2, 3, 5, 7, 14, 21, 30]:\n",
    "    df_features[f'Price_Lag_{lag}'] = df_features['Price'].shift(lag)\n",
    "\n",
    "print(\"Lag features created!\")\n",
    "print(f\"Columns: {[col for col in df_features.columns if 'Lag' in col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Moving Averages (trend indicators)\n",
    "for window in [7, 14, 30, 60, 90]:\n",
    "    df_features[f'MA_{window}'] = df_features['Price'].rolling(window=window).mean()\n",
    "\n",
    "print(\"Moving average features created!\")\n",
    "print(f\"Columns: {[col for col in df_features.columns if 'MA_' in col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Price Returns (percentage change)\n",
    "df_features['Return_1d'] = df_features['Price'].pct_change(1) * 100\n",
    "df_features['Return_5d'] = df_features['Price'].pct_change(5) * 100\n",
    "df_features['Return_30d'] = df_features['Price'].pct_change(30) * 100\n",
    "\n",
    "print(\"Return features created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Volatility (rolling standard deviation)\n",
    "df_features['Volatility_7d'] = df_features['Return_1d'].rolling(window=7).std()\n",
    "df_features['Volatility_30d'] = df_features['Return_1d'].rolling(window=30).std()\n",
    "\n",
    "print(\"Volatility features created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Price momentum indicators\n",
    "df_features['Price_vs_MA7'] = (df_features['Price'] / df_features['MA_7'] - 1) * 100\n",
    "df_features['Price_vs_MA30'] = (df_features['Price'] / df_features['MA_30'] - 1) * 100\n",
    "\n",
    "# 7. Rolling min/max (support/resistance levels)\n",
    "df_features['Rolling_Min_30'] = df_features['Price'].rolling(window=30).min()\n",
    "df_features['Rolling_Max_30'] = df_features['Price'].rolling(window=30).max()\n",
    "\n",
    "print(\"Momentum and support/resistance features created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all features\n",
    "print(f\"Total features: {len(df_features.columns)}\")\n",
    "print(f\"\\nAll columns:\")\n",
    "print(df_features.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values created by feature engineering\n",
    "print(\"Missing values after feature engineering:\")\n",
    "missing = df_features.isnull().sum()\n",
    "print(missing[missing > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Handle Missing Values from Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN (first ~90 days due to rolling windows)\n",
    "df_clean = df_features.dropna().reset_index(drop=True)\n",
    "\n",
    "print(f\"Shape before cleaning: {df_features.shape}\")\n",
    "print(f\"Shape after cleaning: {df_clean.shape}\")\n",
    "print(f\"Rows removed: {len(df_features) - len(df_clean)}\")\n",
    "\n",
    "# Verify no missing values\n",
    "print(f\"\\nRemaining missing values: {df_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Define Target Variable\n",
    "\n",
    "We will predict the **next day's price** (Price at t+1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target: next day's price\n",
    "df_clean['Target'] = df_clean['Price'].shift(-1)\n",
    "\n",
    "# Remove last row (no target available)\n",
    "df_clean = df_clean.dropna().reset_index(drop=True)\n",
    "\n",
    "print(f\"Final dataset shape: {df_clean.shape}\")\n",
    "print(f\"\\nTarget variable: Price at t+1\")\n",
    "print(f\"Target range: {df_clean['Target'].min():.2f} - {df_clean['Target'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 Train-Test Split (Time Series)\n",
    "\n",
    "**Important:** For time series, we must respect temporal order. No random shuffling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define split ratio (80% train, 20% test)\n",
    "train_size = 0.8\n",
    "split_idx = int(len(df_clean) * train_size)\n",
    "\n",
    "# Split data\n",
    "train_data = df_clean.iloc[:split_idx].copy()\n",
    "test_data = df_clean.iloc[split_idx:].copy()\n",
    "\n",
    "print(f\"Training set: {len(train_data)} samples ({len(train_data)/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(test_data)} samples ({len(test_data)/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"\\nTraining period: {train_data['Date'].min()} to {train_data['Date'].max()}\")\n",
    "print(f\"Test period: {test_data['Date'].min()} to {test_data['Date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize train-test split\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(train_data['Date'], train_data['Price'], label='Training Set', color='blue')\n",
    "plt.plot(test_data['Date'], test_data['Price'], label='Test Set', color='orange')\n",
    "plt.axvline(x=train_data['Date'].iloc[-1], color='red', linestyle='--', label='Split Point')\n",
    "plt.title('Train-Test Split Visualization')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Gold Price (USD)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/train_test_split.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns (exclude Date and Target)\n",
    "feature_cols = [col for col in df_clean.columns if col not in ['Date', 'Target']]\n",
    "\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"\\nFeatures: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "X_train = train_data[feature_cols].values\n",
    "y_train = train_data['Target'].values\n",
    "\n",
    "X_test = test_data[feature_cols].values\n",
    "y_test = test_data['Target'].values\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features using StandardScaler\n",
    "# IMPORTANT: Fit only on training data to avoid data leakage!\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled using StandardScaler\")\n",
    "print(f\"\\nTraining set - Mean: {X_train_scaled.mean():.6f}, Std: {X_train_scaled.std():.6f}\")\n",
    "print(f\"Test set - Mean: {X_test_scaled.mean():.6f}, Std: {X_test_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11 Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed datasets\n",
    "train_data.to_csv('../data/processed/train_data.csv', index=False)\n",
    "test_data.to_csv('../data/processed/test_data.csv', index=False)\n",
    "\n",
    "# Save full processed dataset\n",
    "df_clean.to_csv('../data/processed/gold_features.csv', index=False)\n",
    "\n",
    "# Save scaled arrays for modeling\n",
    "np.save('../data/processed/X_train_scaled.npy', X_train_scaled)\n",
    "np.save('../data/processed/X_test_scaled.npy', X_test_scaled)\n",
    "np.save('../data/processed/y_train.npy', y_train)\n",
    "np.save('../data/processed/y_test.npy', y_test)\n",
    "\n",
    "# Save feature names\n",
    "pd.Series(feature_cols).to_csv('../data/processed/feature_names.csv', index=False)\n",
    "\n",
    "# Save scaler for later use\n",
    "import joblib\n",
    "joblib.dump(scaler, '../data/processed/scaler.pkl')\n",
    "\n",
    "print(\"All processed data saved successfully!\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"- ../data/processed/train_data.csv\")\n",
    "print(\"- ../data/processed/test_data.csv\")\n",
    "print(\"- ../data/processed/gold_features.csv\")\n",
    "print(\"- ../data/processed/X_train_scaled.npy\")\n",
    "print(\"- ../data/processed/X_test_scaled.npy\")\n",
    "print(\"- ../data/processed/y_train.npy\")\n",
    "print(\"- ../data/processed/y_test.npy\")\n",
    "print(\"- ../data/processed/feature_names.csv\")\n",
    "print(\"- ../data/processed/scaler.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.12 Summary\n",
    "\n",
    "### Preprocessing Steps Completed:\n",
    "\n",
    "1. **Data Loading:** Loaded daily gold price dataset\n",
    "2. **Missing Values:** Handled using forward-fill method\n",
    "3. **Outlier Analysis:** Identified but kept (valid market data)\n",
    "4. **Feature Engineering:**\n",
    "   - Temporal features (Year, Month, Day, DayOfWeek, Quarter)\n",
    "   - Lag features (1, 2, 3, 5, 7, 14, 21, 30 days)\n",
    "   - Moving averages (7, 14, 30, 60, 90 days)\n",
    "   - Returns (1d, 5d, 30d)\n",
    "   - Volatility (7d, 30d)\n",
    "   - Momentum indicators\n",
    "5. **Target Variable:** Next day's price (t+1)\n",
    "6. **Train-Test Split:** 80/20 chronological split\n",
    "7. **Feature Scaling:** StandardScaler (fit on train only)\n",
    "\n",
    "### Ready for Modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*50)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Original samples: {len(df)}\")\n",
    "print(f\"Final samples: {len(df_clean)}\")\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"\\nTarget: Gold Price at t+1 (USD)\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}