{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Advanced Models - Gold Price Forecasting\n",
    "\n",
    "**Objective:** Implement advanced machine learning models and deep learning.\n",
    "\n",
    "**Author:** Félix Jouary  \n",
    "**Dataset:** Kaggle Gold Price Dataset\n",
    "\n",
    "**Models in this notebook:**\n",
    "- XGBoost (eXtreme Gradient Boosting)\n",
    "- LightGBM\n",
    "- LSTM (Long Short-Term Memory) - Deep Learning\n",
    "\n",
    "**References:**\n",
    "- Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. KDD '16.\n",
    "- Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Advanced ML models\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scaled data\n",
    "X_train_scaled = np.load('../data/processed/X_train_scaled.npy')\n",
    "X_test_scaled = np.load('../data/processed/X_test_scaled.npy')\n",
    "y_train = np.load('../data/processed/y_train.npy')\n",
    "y_test = np.load('../data/processed/y_test.npy')\n",
    "\n",
    "# Load feature names\n",
    "feature_names = pd.read_csv('../data/processed/feature_names.csv').iloc[:, 0].tolist()\n",
    "\n",
    "# Load original data for visualization\n",
    "train_data = pd.read_csv('../data/processed/train_data.csv', parse_dates=['Date'])\n",
    "test_data = pd.read_csv('../data/processed/test_data.csv', parse_dates=['Date'])\n",
    "\n",
    "print(f\"X_train shape: {X_train_scaled.shape}\")\n",
    "print(f\"X_test shape: {X_test_scaled.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"Calculate and display regression metrics.\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{model_name} - Evaluation Metrics\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"RMSE:  ${rmse:.2f}\")\n",
    "    print(f\"MAE:   ${mae:.2f}\")\n",
    "    print(f\"MAPE:  {mape:.2f}%\")\n",
    "    print(f\"R²:    {r2:.4f}\")\n",
    "    \n",
    "    return {'Model': model_name, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape, 'R2': r2}\n",
    "\n",
    "# Store results\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Model 1: XGBoost\n",
    "\n",
    "XGBoost (eXtreme Gradient Boosting) is an optimized gradient boosting algorithm known for:\n",
    "- Regularization to prevent overfitting\n",
    "- Parallel processing for speed\n",
    "- Handling missing values\n",
    "- Built-in cross-validation\n",
    "\n",
    "**Reference:** Chen & Guestrin (2016) - XGBoost: A Scalable Tree Boosting System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series Cross-Validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# XGBoost hyperparameter tuning\n",
    "xgb_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(\n",
    "    XGBRegressor(random_state=42, n_jobs=-1, verbosity=0),\n",
    "    xgb_params,\n",
    "    cv=tscv,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training XGBoost with GridSearchCV...\")\n",
    "xgb_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {xgb_grid.best_params_}\")\n",
    "print(f\"Best CV score (neg MSE): {xgb_grid.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with best parameters\n",
    "xgb_model = xgb_grid.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgb_train = xgb_model.predict(X_train_scaled)\n",
    "y_pred_xgb_test = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "print(\"TRAINING SET:\")\n",
    "_ = evaluate_model(y_train, y_pred_xgb_train, \"XGBoost (Train)\")\n",
    "\n",
    "print(\"\\nTEST SET:\")\n",
    "xgb_results = evaluate_model(y_test, y_pred_xgb_test, \"XGBoost\")\n",
    "results.append(xgb_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Feature Importance\n",
    "xgb_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(xgb_importance['Feature'].head(15)[::-1], xgb_importance['Importance'].head(15)[::-1])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 15 Feature Importances - XGBoost')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/xgb_feature_importance.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 most important features (XGBoost):\")\n",
    "print(xgb_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Model 2: LightGBM\n",
    "\n",
    "LightGBM is a gradient boosting framework that uses tree-based learning algorithms. It is:\n",
    "- Faster than XGBoost\n",
    "- Uses less memory\n",
    "- Better accuracy with large datasets\n",
    "\n",
    "**Reference:** Ke et al. (2017) - LightGBM: A Highly Efficient Gradient Boosting Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM hyperparameter tuning\n",
    "lgbm_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7, -1],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'num_leaves': [31, 50, 100]\n",
    "}\n",
    "\n",
    "lgbm_grid = GridSearchCV(\n",
    "    LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1),\n",
    "    lgbm_params,\n",
    "    cv=tscv,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training LightGBM with GridSearchCV...\")\n",
    "lgbm_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {lgbm_grid.best_params_}\")\n",
    "print(f\"Best CV score (neg MSE): {lgbm_grid.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with best parameters\n",
    "lgbm_model = lgbm_grid.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_pred_lgbm_train = lgbm_model.predict(X_train_scaled)\n",
    "y_pred_lgbm_test = lgbm_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "print(\"TRAINING SET:\")\n",
    "_ = evaluate_model(y_train, y_pred_lgbm_train, \"LightGBM (Train)\")\n",
    "\n",
    "print(\"\\nTEST SET:\")\n",
    "lgbm_results = evaluate_model(y_test, y_pred_lgbm_test, \"LightGBM\")\n",
    "results.append(lgbm_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Model 3: LSTM (Deep Learning)\n",
    "\n",
    "LSTM (Long Short-Term Memory) is a type of recurrent neural network (RNN) designed to learn long-term dependencies in sequential data.\n",
    "\n",
    "**Why LSTM for time series:**\n",
    "- Captures temporal patterns\n",
    "- Handles variable-length sequences\n",
    "- Memory cells retain information over time\n",
    "\n",
    "**Reference:** Hochreiter & Schmidhuber (1997) - Long Short-Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM (needs 3D input: samples, timesteps, features)\n",
    "# We'll use a sequence length of 30 days\n",
    "\n",
    "def create_sequences(X, y, seq_length=30):\n",
    "    \"\"\"Create sequences for LSTM input.\"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(seq_length, len(X)):\n",
    "        X_seq.append(X[i-seq_length:i])\n",
    "        y_seq.append(y[i])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Create sequences\n",
    "SEQ_LENGTH = 30\n",
    "\n",
    "X_train_lstm, y_train_lstm = create_sequences(X_train_scaled, y_train, SEQ_LENGTH)\n",
    "X_test_lstm, y_test_lstm = create_sequences(X_test_scaled, y_test, SEQ_LENGTH)\n",
    "\n",
    "print(f\"LSTM Training shape: {X_train_lstm.shape}\")\n",
    "print(f\"LSTM Test shape: {X_test_lstm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM model\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(25, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "lstm_model = build_lstm_model((SEQ_LENGTH, X_train_scaled.shape[1]))\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM with early stopping\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "print(\"Training LSTM model...\")\n",
    "history = lstm_model.fit(\n",
    "    X_train_lstm, y_train_lstm,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining stopped at epoch {len(history.history['loss'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Training Loss')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axes[0].set_title('LSTM Training History - Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].legend()\n",
    "\n",
    "# MAE\n",
    "axes[1].plot(history.history['mae'], label='Training MAE')\n",
    "axes[1].plot(history.history['val_mae'], label='Validation MAE')\n",
    "axes[1].set_title('LSTM Training History - MAE')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/lstm_training_history.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Predictions\n",
    "y_pred_lstm_train = lstm_model.predict(X_train_lstm, verbose=0).flatten()\n",
    "y_pred_lstm_test = lstm_model.predict(X_test_lstm, verbose=0).flatten()\n",
    "\n",
    "# Evaluate\n",
    "print(\"TRAINING SET:\")\n",
    "_ = evaluate_model(y_train_lstm, y_pred_lstm_train, \"LSTM (Train)\")\n",
    "\n",
    "print(\"\\nTEST SET:\")\n",
    "lstm_results = evaluate_model(y_test_lstm, y_pred_lstm_test, \"LSTM\")\n",
    "results.append(lstm_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Overfitting Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare train vs test performance\n",
    "models_names = ['XGBoost', 'LightGBM', 'LSTM']\n",
    "train_preds = [y_pred_xgb_train, y_pred_lgbm_train, y_pred_lstm_train]\n",
    "test_preds = [y_pred_xgb_test, y_pred_lgbm_test, y_pred_lstm_test]\n",
    "y_trains = [y_train, y_train, y_train_lstm]\n",
    "y_tests = [y_test, y_test, y_test_lstm]\n",
    "\n",
    "overfitting_analysis = []\n",
    "for name, train_pred, test_pred, y_tr, y_te in zip(models_names, train_preds, test_preds, y_trains, y_tests):\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_tr, train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_te, test_pred))\n",
    "    gap = ((test_rmse - train_rmse) / train_rmse) * 100\n",
    "    \n",
    "    overfitting_analysis.append({\n",
    "        'Model': name,\n",
    "        'Train RMSE': train_rmse,\n",
    "        'Test RMSE': test_rmse,\n",
    "        'Gap (%)': gap\n",
    "    })\n",
    "\n",
    "overfit_df = pd.DataFrame(overfitting_analysis)\n",
    "print(\"Overfitting Analysis - Advanced Models:\")\n",
    "print(overfit_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('RMSE')\n",
    "\n",
    "print(\"Advanced Models Comparison (sorted by RMSE):\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "colors = ['#2ecc71', '#3498db', '#9b59b6']\n",
    "\n",
    "# RMSE\n",
    "axes[0, 0].barh(results_df['Model'], results_df['RMSE'], color=colors)\n",
    "axes[0, 0].set_xlabel('RMSE ($)')\n",
    "axes[0, 0].set_title('RMSE by Model (lower is better)')\n",
    "\n",
    "# MAE\n",
    "axes[0, 1].barh(results_df['Model'], results_df['MAE'], color=colors)\n",
    "axes[0, 1].set_xlabel('MAE ($)')\n",
    "axes[0, 1].set_title('MAE by Model (lower is better)')\n",
    "\n",
    "# MAPE\n",
    "axes[1, 0].barh(results_df['Model'], results_df['MAPE'], color=colors)\n",
    "axes[1, 0].set_xlabel('MAPE (%)')\n",
    "axes[1, 0].set_title('MAPE by Model (lower is better)')\n",
    "\n",
    "# R²\n",
    "axes[1, 1].barh(results_df['Model'], results_df['R2'], color=colors)\n",
    "axes[1, 1].set_xlabel('R²')\n",
    "axes[1, 1].set_title('R² by Model (higher is better)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/model_comparison_advanced.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9 Predictions Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions for all advanced models\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "# XGBoost\n",
    "axes[0].plot(test_data['Date'], y_test, label='Actual', alpha=0.8)\n",
    "axes[0].plot(test_data['Date'], y_pred_xgb_test, label='XGBoost', alpha=0.8)\n",
    "axes[0].set_title('XGBoost Predictions')\n",
    "axes[0].set_ylabel('Price (USD)')\n",
    "axes[0].legend()\n",
    "\n",
    "# LightGBM\n",
    "axes[1].plot(test_data['Date'], y_test, label='Actual', alpha=0.8)\n",
    "axes[1].plot(test_data['Date'], y_pred_lgbm_test, label='LightGBM', alpha=0.8)\n",
    "axes[1].set_title('LightGBM Predictions')\n",
    "axes[1].set_ylabel('Price (USD)')\n",
    "axes[1].legend()\n",
    "\n",
    "# LSTM (note: shorter due to sequence creation)\n",
    "lstm_dates = test_data['Date'].iloc[SEQ_LENGTH:].reset_index(drop=True)\n",
    "axes[2].plot(lstm_dates, y_test_lstm, label='Actual', alpha=0.8)\n",
    "axes[2].plot(lstm_dates, y_pred_lstm_test, label='LSTM', alpha=0.8)\n",
    "axes[2].set_title('LSTM Predictions')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].set_ylabel('Price (USD)')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/advanced_models_predictions.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.10 Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "joblib.dump(xgb_model, '../models/xgboost.pkl')\n",
    "joblib.dump(lgbm_model, '../models/lightgbm.pkl')\n",
    "lstm_model.save('../models/lstm_model.keras')\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('../reports/advanced_results.csv', index=False)\n",
    "\n",
    "print(\"All models saved successfully!\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"- ../models/xgboost.pkl\")\n",
    "print(\"- ../models/lightgbm.pkl\")\n",
    "print(\"- ../models/lstm_model.keras\")\n",
    "print(\"- ../reports/advanced_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.11 Summary\n",
    "\n",
    "### Advanced Models Implemented:\n",
    "\n",
    "1. **XGBoost** - State-of-the-art gradient boosting\n",
    "   - Reference: Chen & Guestrin (2016)\n",
    "   - Regularization, parallel processing, handles missing values\n",
    "\n",
    "2. **LightGBM** - Fast gradient boosting framework\n",
    "   - Reference: Ke et al. (2017)\n",
    "   - Faster and more memory efficient than XGBoost\n",
    "\n",
    "3. **LSTM** - Deep learning for sequences\n",
    "   - Reference: Hochreiter & Schmidhuber (1997)\n",
    "   - Captures long-term temporal dependencies\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "- All models use **TimeSeriesSplit** for cross-validation\n",
    "- LSTM uses **Early Stopping** to prevent overfitting\n",
    "- XGBoost and LightGBM typically outperform traditional ML models\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Compare all models (baseline + advanced)\n",
    "- Select best model for production\n",
    "- Final conclusions and recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*60)\n",
    "print(\"ADVANCED MODELS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBest Advanced Model: {results_df.iloc[0]['Model']}\")\n",
    "print(f\"Best RMSE: ${results_df.iloc[0]['RMSE']:.2f}\")\n",
    "print(f\"Best MAE: ${results_df.iloc[0]['MAE']:.2f}\")\n",
    "print(f\"Best MAPE: {results_df.iloc[0]['MAPE']:.2f}%\")\n",
    "print(f\"Best R²: {results_df.iloc[0]['R2']:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
